{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RYFaw-lYaAb3"},"outputs":[],"source":["import argparse\n","import pickle\n","from tqdm import tqdm\n","import sys\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","sys.path.extend(['../'])\n","\n","\n","training_subjects = [1, 2, 3, 4, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 38, 40, 41, 42, 43]\n","num_joint = 28\n","max_frame = 50\n","\n","import json\n","\n","def read_skeleton_filter(file):\n","    # Read JSON file\n","    with open(file, 'r') as f:\n","        data = json.load(f)\n","\n","    # Calculate number of frames and joints per frame\n","    num_frames = len(data['annotations'])\n","    num_joints = data['annotations'][0]['num_keypoints']\n","\n","    skeleton_sequence = {\n","        'numFrame': num_frames,\n","        'frameInfo': []\n","    }\n","\n","    # Iterate over each frame\n","    for annotation in data['annotations']:\n","        frame_info = {\n","            'numJoint': num_joints,\n","            'jointInfo': []\n","        }\n","\n","        keypoints = annotation['keypoints']\n","        for i in range(0, len(keypoints), 3):\n","            x = keypoints[i]\n","            y = keypoints[i + 1]\n","\n","            joint_info = {\n","                'x': x,\n","                'y': y\n","            }\n","            frame_info['jointInfo'].append(joint_info)\n","\n","        skeleton_sequence['frameInfo'].append(frame_info)\n","\n","    return skeleton_sequence\n","\n","def read_xy(file, num_joint=28):\n","    seq_info = read_skeleton_filter(file)\n","    data = np.zeros((seq_info['numFrame'], num_joint, 2))\n","\n","    for n, f in enumerate(seq_info['frameInfo']):\n","        for j, v in enumerate(f['jointInfo']):\n","            data[n, j, :] = [v['x'], v['y']]  # Include only X and Y coordinates\n","\n","    data = data.transpose(2, 0, 1)  # Transpose for desired output format (X,Y first)\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwwrWorwaAb5","outputId":"ec1253c9-4f4b-4f3e-8c14-24a922abd150"},"outputs":[{"name":"stdout","output_type":"stream","text":["xsub train\n","Processing file: S001P001A001.json\n","Processing file: S002P002A001.json\n","Processing file: S003P003A001.json\n","Processing file: S004P004A001.json\n","Processing file: S005P005A001.json\n","Processing file: S006P006A001.json\n","Processing file: S007P007A001.json\n","Processing file: S008P008A001.json\n","Processing file: S009P009A002.json\n","Processing file: S010P010A002.json\n","Processing file: S011P011A002.json\n","Processing file: S012P012A002.json\n","Processing file: S013P013A003.json\n","Processing file: S014P014A003.json\n","Processing file: S015P015A003.json\n","Processing file: S016P016A003.json\n","Processing file: S017P017A003.json\n","Processing file: S018P018A003.json\n","Processing file: S019P019A001.json\n","Processing file: S020P020A001.json\n","Processing file: S021P021A003.json\n","Processing file: S022P022A001.json\n","Processing file: S023P023A003.json\n","Processing file: S024P024A001.json\n","Processing file: S025P025A003.json\n","Processing file: S026P026A002.json\n","Processing file: S027P027A002.json\n","Processing file: S028P028A001.json\n","Processing file: S029P029A002.json\n","Processing file: S030P030A001.json\n","Processing file: S031P031A003.json\n","Processing file: S032P032A002.json\n","Processing file: S033P033A003.json\n","Processing file: S034P034A002.json\n","Processing file: S035P035A002.json\n","Processing file: S036P036A001.json\n","Processing file: S037P037A002.json\n","Processing file: S038P038A003.json\n","Processing file: S039P039A002.json\n","Processing file: S040P040A002.json\n","Processing file: S041P041A003.json\n","Processing file: S042P042A003.json\n","Processing file: S043P043A003.json\n"]},{"name":"stderr","output_type":"stream","text":[" 41%|████      | 14/34 [00:00<00:00, 130.76it/s]"]},{"name":"stdout","output_type":"stream","text":["Processing file: S001P001A001.json, data shape: (2, 197, 28)\n","Processing file: S002P002A001.json, data shape: (2, 129, 28)\n","Processing file: S003P003A001.json, data shape: (2, 168, 28)\n","Processing file: S004P004A001.json, data shape: (2, 102, 28)\n","Processing file: S006P006A001.json, data shape: (2, 181, 28)\n","Processing file: S007P007A001.json, data shape: (2, 123, 28)\n","Processing file: S008P008A001.json, data shape: (2, 180, 28)\n","Processing file: S010P010A002.json, data shape: (2, 187, 28)\n","Processing file: S011P011A002.json, data shape: (2, 182, 28)\n","Processing file: S012P012A002.json, data shape: (2, 191, 28)\n","Processing file: S013P013A003.json, data shape: (2, 218, 28)\n","Processing file: S014P014A003.json, data shape: (2, 126, 28)\n","Processing file: S015P015A003.json, data shape: (2, 211, 28)\n","Processing file: S016P016A003.json, data shape: (2, 182, 28)\n","Processing file: S018P018A003.json, data shape: (2, 210, 28)\n","Processing file: S019P019A001.json, data shape: (2, 200, 28)\n","Processing file: S020P020A001.json, data shape: (2, 190, 28)\n","Processing file: S021P021A003.json, data shape: (2, 176, 28)\n","Processing file: S022P022A001.json, data shape: (2, 191, 28)\n","Processing file: S026P026A002.json, data shape: (2, 196, 28)\n","Processing file: S027P027A002.json, data shape: (2, 164, 28)\n","Processing file: S028P028A001.json, data shape: (2, 203, 28)\n","Processing file: S029P029A002.json, data shape: (2, 207, 28)\n","Processing file: S031P031A003.json, data shape: (2, 214, 28)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 34/34 [00:00<00:00, 114.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing file: S032P032A002.json, data shape: (2, 158, 28)\n","Processing file: S033P033A003.json, data shape: (2, 114, 28)\n","Processing file: S034P034A002.json, data shape: (2, 185, 28)\n","Processing file: S035P035A002.json, data shape: (2, 183, 28)\n","Processing file: S036P036A001.json, data shape: (2, 182, 28)\n","Processing file: S038P038A003.json, data shape: (2, 189, 28)\n","Processing file: S040P040A002.json, data shape: (2, 199, 28)\n","Processing file: S041P041A003.json, data shape: (2, 151, 28)\n","Processing file: S042P042A003.json, data shape: (2, 151, 28)\n","Processing file: S043P043A003.json, data shape: (2, 151, 28)\n","Final dataset shape: (137, 2, 50, 28)\n","Final labels shape: (137,)\n","xsub val\n","Processing file: S001P001A001.json\n","Processing file: S002P002A001.json\n","Processing file: S003P003A001.json\n","Processing file: S004P004A001.json\n","Processing file: S005P005A001.json\n","Processing file: S006P006A001.json\n","Processing file: S007P007A001.json\n","Processing file: S008P008A001.json\n","Processing file: S009P009A002.json\n","Processing file: S010P010A002.json\n","Processing file: S011P011A002.json\n","Processing file: S012P012A002.json\n","Processing file: S013P013A003.json\n","Processing file: S014P014A003.json\n","Processing file: S015P015A003.json\n","Processing file: S016P016A003.json\n","Processing file: S017P017A003.json\n","Processing file: S018P018A003.json\n","Processing file: S019P019A001.json\n","Processing file: S020P020A001.json\n","Processing file: S021P021A003.json\n","Processing file: S022P022A001.json\n","Processing file: S023P023A003.json\n","Processing file: S024P024A001.json\n","Processing file: S025P025A003.json\n","Processing file: S026P026A002.json\n","Processing file: S027P027A002.json\n","Processing file: S028P028A001.json\n","Processing file: S029P029A002.json\n","Processing file: S030P030A001.json\n","Processing file: S031P031A003.json\n","Processing file: S032P032A002.json\n","Processing file: S033P033A003.json\n","Processing file: S034P034A002.json\n","Processing file: S035P035A002.json\n","Processing file: S036P036A001.json\n","Processing file: S037P037A002.json\n","Processing file: S038P038A003.json\n","Processing file: S039P039A002.json\n","Processing file: S040P040A002.json\n","Processing file: S041P041A003.json\n","Processing file: S042P042A003.json\n","Processing file: S043P043A003.json\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 9/9 [00:00<00:00, 141.03it/s]"]},{"name":"stdout","output_type":"stream","text":["Processing file: S005P005A001.json, data shape: (2, 109, 28)\n","Processing file: S009P009A002.json, data shape: (2, 136, 28)\n","Processing file: S017P017A003.json, data shape: (2, 91, 28)\n","Processing file: S023P023A003.json, data shape: (2, 189, 28)\n","Processing file: S024P024A001.json, data shape: (2, 204, 28)\n","Processing file: S025P025A003.json, data shape: (2, 188, 28)\n","Processing file: S030P030A001.json, data shape: (2, 188, 28)\n","Processing file: S037P037A002.json, data shape: (2, 191, 28)\n","Processing file: S039P039A002.json, data shape: (2, 208, 28)\n","Final dataset shape: (34, 2, 50, 28)\n","Final labels shape: (34,)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import numpy as np\n","import os\n","from tqdm import tqdm\n","import pickle\n","\n","max_frame = 50  # Number of frames\n","num_joint = 28  # Number of joints\n","\n","def gendata(data_path, out_path, benchmark='xsub', set_name='val'):\n","    sample_name = []\n","    sample_label = []\n","\n","    # Identify sample names and labels\n","    for filename in os.listdir(data_path):\n","        print(f\"Processing file: {filename}\")\n","        action_class = int(filename[filename.find('A') + 1:filename.find('A') + 4])\n","        subject_id = int(filename[filename.find('P') + 1:filename.find('P') + 4])\n","\n","        # Determine if the sample belongs to training or validation set\n","        istraining = (subject_id in training_subjects)\n","        issample = (istraining if set_name == 'train' else not istraining)\n","\n","        if issample:\n","            sample_name.append(filename)\n","            sample_label.append(action_class - 1)  # Convert to 0-based index\n","\n","\n","\n","    # Prepare to store the chunks and corresponding labels\n","    fp_list = []\n","    labels_list = []\n","\n","    # Process each sample\n","    for i, s in enumerate(tqdm(sample_name)):\n","        data = read_xy(os.path.join(data_path, s), num_joint=num_joint)\n","        num_frames = data.shape[1]\n","        print(f\"Processing file: {s}, data shape: {data.shape}\")\n","\n","        # Create chunks of size 50 frames\n","        for start_frame in range(0, num_frames, max_frame):\n","            end_frame = start_frame + max_frame\n","\n","            # Extract chunk of 50 frames or less\n","            if end_frame <= num_frames:\n","                data_chunk = data[:, start_frame:end_frame, :]\n","\n","\n","            fp_list.append(data_chunk)\n","            labels_list.append(sample_label[i])\n","\n","    # Convert the list of chunks to NumPy arrays\n","    fp = np.array(fp_list)\n","    labels = np.array(labels_list)\n","\n","    print(f\"Final dataset shape: {fp.shape}\")\n","    print(f\"Final labels shape: {labels.shape}\")\n","\n","    # Save the processed data and labels\n","    np.save(f'{out_path}/{set_name}_data_joint_pad.npy', fp)\n","    with open(f'{out_path}/{set_name}_labels_pad.pkl', 'wb') as f:\n","        pickle.dump(labels, f)\n","\n","if __name__ == '__main__':\n","    benchmark = ['xsub']\n","    set_name = ['train', 'val']\n","    data_path = r\"/content/drive/MyDrive/PFE/Model/Horse_Gait_Recognition/Data/Raw\"\n","    out_folder = r'/content/drive/MyDrive/PFE/Model/Horse_Gait_Recognition/Data/Extracted/xsub\"\n","\n","    for b in benchmark:\n","        for sn in set_name:\n","            out_path = os.path.join(out_folder, b)\n","            if not os.path.exists(out_path):\n","                os.makedirs(out_path)\n","            print(b, sn)\n","            gendata(data_path, out_path, benchmark=b, set_name=sn)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jcGu35JFaAb6","outputId":"d3bc18e5-22b1-4288-e627-8fda1c0d2ba9"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading data\n","(137, 2, 50, 28)\n","(34, 2, 50, 28)\n","()\n","()\n","(137, 50, 28, 2)\n","(34, 50, 28, 2)\n"]}],"source":["\n","import torch\n","import math\n","import numpy as np\n","import argparse\n","import os\n","import torch.nn as nn\n","\n","\n","# Define directory path for your  data\n","data_dir = r'C:\\Users\\hp\\OneDrive - Efrei\\AD-and-DLB-Classifier-master Baseline+gap\\AD-and-DLB-Classifier-master\\data\\Multiple1\\xsub'\n","\n","# Define filenames for training and testing data (assuming .npy format)\n","file_train = 'train_data_joint_pad.npy'\n","file_test = 'val_data_joint_pad.npy'\n","\n","print('loading data')\n","X_train = np.load(os.path.join(data_dir,file_train),  allow_pickle=True)\n","X_test = np.load(os.path.join(data_dir,file_test), allow_pickle=True)\n","y_train = np.load(os.path.join(data_dir,'train_labels_pad.pkl'), allow_pickle=True)\n","y_test = np.load(os.path.join(data_dir,'val_labels_pad.pkl'), allow_pickle=True)\n","\n","y_train = y_train[1]\n","\n","y_test = y_test[1]\n","\n","y_train = np.array(y_train).astype('int32')\n","y_test = np.array(y_test).astype('int32')\n","\n","print(X_train.shape)\n","print(X_test.shape)\n","print(y_train.shape)\n","print(y_test.shape)\n","\n","X_train = X_train.transpose(0, 2 ,3 , 1)\n","print(X_train.shape)\n","X_test= X_test.transpose(0, 2 ,3 , 1)\n","print(X_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V8kHc-EMaAb7","outputId":"835f66fd-b275-492b-b1a6-9c32902fb76c"},"outputs":[{"name":"stdout","output_type":"stream","text":["(137, 50, 28, 2)\n","(34, 50, 28, 2)\n","(137,)\n","(34,)\n"]}],"source":["import pickle\n","\n","def load_data_labels(data_dir, set_name):\n","\n","    with open(os.path.join(data_dir, f'{set_name}_labels_pad.pkl'), 'rb') as f:\n","        labels = pickle.load(f)\n","    return  labels\n","\n","# Example usage\n","y_train = load_data_labels(data_dir, 'train')\n","y_test = load_data_labels(data_dir, 'val')\n","\n","print(X_train.shape)\n","print(X_test.shape)\n","print(y_train.shape)\n","print(y_test.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJhYj8UnaAb-"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}